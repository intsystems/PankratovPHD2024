{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6830a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/SW')\n",
    "from __future__ import print_function\n",
    "from future.builtins import zip\n",
    "from future.utils import iteritems\n",
    "import random\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from multiprocessing import Pool\n",
    "import pickle\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim import utils\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from scipy import sparse\n",
    "from scipy.stats import dirichlet\n",
    "from scipy.stats import multinomial\n",
    "from scipy.stats import zipf\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.linalg import norm\n",
    "from pyartm import regularizers\n",
    "from pyartm.optimizations import gradient\n",
    "from pyartm.optimizations import default\n",
    "from pyartm.common import experiments\n",
    "from pyartm_experiments.common import default_plot  \n",
    "import pyartm_datasets\n",
    "from pyartm_datasets import main_cases\n",
    "from pyartm_datasets import sklearn_dataset\n",
    "from pyartm_datasets import nips\n",
    "from pyartm_datasets import twitter_sentiment140\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from pyartm.optimizations import balanced\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78174f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/SW')\n",
    "from __future__ import print_function\n",
    "from future.builtins import zip\n",
    "from future.utils import iteritems\n",
    "import random\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from multiprocessing import Pool\n",
    "import pickle\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim import utils\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from scipy import sparse\n",
    "from scipy.stats import dirichlet\n",
    "from scipy.stats import multinomial\n",
    "from scipy.stats import zipf\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.linalg import norm\n",
    "from pyartm import regularizers\n",
    "from pyartm.optimizations import gradient\n",
    "from pyartm.optimizations import default\n",
    "from pyartm.common import experiments\n",
    "from pyartm_experiments.common import default_plot  \n",
    "import pyartm_datasets\n",
    "from pyartm_datasets import main_cases\n",
    "from pyartm_datasets import sklearn_dataset\n",
    "from pyartm_datasets import nips\n",
    "from pyartm_datasets import twitter_sentiment140\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from pyartm.optimizations import balanced\n",
    "%matplotlib inline\n",
    "\n",
    "EPS = 2**(-15)\n",
    "DOCS = 2000\n",
    "WORDS = 10000\n",
    "WORDS_IN_DOC = 1000\n",
    "TOPICS = 100\n",
    "ARTM_DIR = os.path.dirname(os.path.realpath(pyartm_datasets.__file__))\n",
    "ARTM_RESOURCES = os.path.join(ARTM_DIR, 'resources')\n",
    "DATASETS_PATH = os.environ.get('PYARTM_DATASETS_PATH', os.path.join(os.path.expanduser('~'), 'pyartm-datasets'))\n",
    "NIPS_PATH = os.path.join(DATASETS_PATH, 'NIPS.csv')\n",
    "TWITTER_SENTIMENT140_PATH = os.path.join( DATASETS_PATH, 'twitter-sentiment140.csv')\n",
    "WNTM_MATRIX_DIR_PATH = os.path.join(DATASETS_PATH, 'wntm_matrix')\n",
    "ITERS_COUNT = 401\n",
    "SAMPLES = 1\n",
    "DEFAULT_EXP_PATH = 'drive/MyDrive/experiment.pkl'\n",
    "DEFAUL_EXP_PHI_PATH = 'drive/MyDrive/experimentphi.pkl'\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "def prepare(\n",
    "        dataset,\n",
    "        train_proportion=None,\n",
    "        min_occurrences = 3,\n",
    "        token_2_num = None,\n",
    "        process_log_step = 500\n",
    "):\n",
    "    english_stopwords = set(stopwords.words('english'))\n",
    "    is_token_2_num_provided = token_2_num is not None\n",
    "    occurrences = None\n",
    "\n",
    "    # remove stopwords\n",
    "    if not is_token_2_num_provided:\n",
    "        token_2_num = dict()\n",
    "        occurrences = Counter()\n",
    "        for i, doc in enumerate(dataset.data):\n",
    "            tokens = gensim.utils.lemmatize(doc)\n",
    "            for token in set(tokens):\n",
    "                occurrences[token] += 1\n",
    "            if i % process_log_step == 0:\n",
    "                print('Preprocessed: ', i, 'documents from', len(dataset.data))\n",
    "\n",
    "    row, col, data = [], [], []\n",
    "    row_test, col_test, data_test = [], [], []\n",
    "    not_empty_docs_number = 0\n",
    "    doc_targets = []\n",
    "    random_gen = random.Random(42)\n",
    "\n",
    "    for i, (doc, target) in enumerate(\n",
    "        zip(dataset.data, dataset.target)\n",
    "    ):\n",
    "        tokens = gensim.utils.lemmatize(doc)\n",
    "        cnt = Counter()\n",
    "        cnt_test = Counter()\n",
    "        for token in (set(tokens)):\n",
    "            word = (token.decode(\"utf-8\")).split('/')[0]\n",
    "            if (\n",
    "                    not is_token_2_num_provided\n",
    "                    and word not in english_stopwords\n",
    "                    and min_occurrences <= occurrences[token]\n",
    "                    and token not in token_2_num\n",
    "            ):\n",
    "                token_2_num[token] = len(token_2_num)\n",
    "            if token in token_2_num:\n",
    "                if (\n",
    "                        train_proportion is None\n",
    "                        or random_gen.random() < train_proportion\n",
    "                ):\n",
    "                    cnt[token_2_num[token]] += 1\n",
    "                else:\n",
    "                    cnt_test[token_2_num[token]] += 1\n",
    "\n",
    "        if len(cnt) > 0 and (train_proportion is None or len(cnt_test) > 0):\n",
    "            for w, c in iteritems(cnt):\n",
    "                row.append(not_empty_docs_number)\n",
    "                col.append(w)\n",
    "                data.append(c)\n",
    "\n",
    "            for w, c in iteritems(cnt_test):\n",
    "                row_test.append(not_empty_docs_number)\n",
    "                col_test.append(w)\n",
    "                data_test.append(c)\n",
    "\n",
    "            not_empty_docs_number += 1\n",
    "            doc_targets.append(target)\n",
    "\n",
    "        if i % process_log_step == 0:\n",
    "            print('Processed: ', i, 'documents from', len(dataset.data))\n",
    "\n",
    "    num_2_token = {\n",
    "        v: k\n",
    "        for k, v in iteritems(token_2_num)\n",
    "    }\n",
    "\n",
    "    shape = (not_empty_docs_number, len(token_2_num))\n",
    "    if train_proportion is None:\n",
    "        return (\n",
    "            scipy.sparse.csr_matrix((data, (row, col)), shape=shape),\n",
    "            token_2_num,\n",
    "            num_2_token,\n",
    "            doc_targets\n",
    "        )\n",
    "    else:\n",
    "        return (\n",
    "            scipy.sparse.csr_matrix((data, (row, col)), shape=shape),\n",
    "            scipy.sparse.csr_matrix(\n",
    "                (data_test, (row_test, col_test)), shape=shape\n",
    "            ),\n",
    "            token_2_num,\n",
    "            num_2_token,\n",
    "            doc_targets\n",
    "        )\n",
    "def get_resource_path(name):\n",
    "    return os.path.join(ARTM_RESOURCES, name)\n",
    "    \n",
    "def get_20newsgroups(categories = None, min_occurrences=3, train_proportion=None, subset='all'):\n",
    "    path = get_resource_path('20newsgroups_subset_{}_{}_{}.pkl'.format(\n",
    "        subset, min_occurrences, train_proportion))\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        with open(path, 'rb') as resource_file:\n",
    "            data = pickle.load(resource_file)\n",
    "    else:\n",
    "        data = prepare(\n",
    "            fetch_20newsgroups(\n",
    "                subset=subset,\n",
    "                categories=categories,\n",
    "                remove=('headers', 'footers', 'quotes'), random_state= 100\n",
    "            ),\n",
    "            min_occurrences=min_occurrences,\n",
    "            train_proportion=train_proportion\n",
    "        )\n",
    "        with open(path, 'wb') as resource_file:\n",
    "            pickle.dump(data, resource_file)\n",
    "    return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d182967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simil_fun_cdist(a, b):\n",
    "    return ( cdist(a.reshape(1,len(a)), b.reshape(1,len(b)),'cosine'))\n",
    "\n",
    "def simil_fun_euclid(a, b):\n",
    "    return (np.sum((a - b) * (a - b)))\n",
    "\n",
    "def simil_fun_hellinger(a, b):\n",
    "    return (norm(np.sqrt(a) - np.sqrt(b)) / np.sqrt(2))\n",
    "\n",
    "def simil_fun_jaccard_5(a, b):\n",
    "    a_bin = a >= 10**(-5)\n",
    "    b_bin = b >= 10**(-5)\n",
    "    return scipy.spatial.distance.jaccard(a, b)\n",
    "\n",
    "def simil_fun_jaccard_4(a,b):\n",
    "    a_bin = a >= 10**(-4)\n",
    "    b_bin = b >= 10**(-4)\n",
    "    return scipy.spatial.distance.jaccard(a, b)\n",
    "\n",
    "def simil_fun_jenson_shennon(a, b):\n",
    "    return scipy.spatial.distance.jensenshannon(a, b)\n",
    "\n",
    "def count_metrics_for_phi(phi, phi_0, simil_fun, num_topics):\n",
    "    distance = np.zeros((num_topics,num_topics))\n",
    "    for i in range(num_topics):\n",
    "        for j in range(num_topics):\n",
    "            distance[i][j] = simil_fun(phi[:,i],phi_0[:,j])\n",
    "    ax0 = np.argmin(distance, axis = 0)\n",
    "    ax1 = np.argmin(distance, axis = 1)\n",
    "    uni0 = np.unique(ax0, return_counts = True)\n",
    "    uni1 = np.unique(ax1, return_counts = True)\n",
    "    f1 = pairsim(ax0, ax1)\n",
    "    f2 = (np.sum((uni0[1] > 1))) # \n",
    "    f3 = (np.sum((uni1[1] > 1))) #\n",
    "    f4 = (num_topics - len(uni0[0]))\n",
    "    f5 = (num_topics - len(uni1[0]))\n",
    "    return(f1, f2, f3, f4, f5)\n",
    "\n",
    "def count_metrics_values_for_phi(phi, phi_0, simil_fun, num_topics):\n",
    "    distance = np.zeros((num_topics,num_topics))\n",
    "    for i in range(num_topics):\n",
    "        for j in range(num_topics):\n",
    "            distance[i][j] = simil_fun(phi[:,i],phi_0[:,j])\n",
    "    ax0 = np.min(distance, axis = 0)\n",
    "    return np.sum(ax0)\n",
    "\n",
    "def check(phi_true, phi_test, num_topics):\n",
    "    m1 = count_metrics_for_phi(phi_true, phi_test, simil_fun_cdist, num_topics)\n",
    "    m2 = count_metrics_for_phi(phi_true, phi_test, simil_fun_euclid, num_topics)\n",
    "    m3 = count_metrics_for_phi(phi_true, phi_test, simil_fun_jenson_shennon, num_topics)\n",
    "    m4 = count_metrics_for_phi(phi_true, phi_test, simil_fun_hellinger, num_topics)\n",
    "    print(m1)\n",
    "    print(m2) \n",
    "    print(m3) \n",
    "    print(m4)\n",
    "    return(m1,m2,m3,m4)\n",
    "\n",
    "def asym_count_metrics_values_for_phi(phi, phi_0, simil_fun, num_topics_phi, num_topics_phi_0):\n",
    "    distance = np.zeros((num_topics_phi,num_topics_phi_0))\n",
    "    for i in range(num_topics_phi):\n",
    "        for j in range(num_topics_phi_0):\n",
    "            distance[i][j] = simil_fun(phi[:,i],phi_0[:,j])\n",
    "    ax0 = np.min(distance, axis = 0)\n",
    "    return np.sum(ax0)\n",
    "def asym_count_metrics_for_phi(phi, phi_0, simil_fun,num_topics_phi, num_topics_phi_0):\n",
    "    distance = np.zeros((num_topics_phi,num_topics_phi_0))\n",
    "    for i in range(num_topics_phi):\n",
    "        for j in range(num_topics_phi_0):\n",
    "            distance[i][j] = simil_fun(phi[:,i],phi_0[:,j])\n",
    "    ax0 = np.argmin(distance, axis = 0)\n",
    "    ax1 = np.argmin(distance, axis = 1)\n",
    "    uni0 = np.unique(ax0, return_counts = True)\n",
    "    uni1 = np.unique(ax1, return_counts = True)\n",
    "    f1 = pairsim(ax0, ax1)\n",
    "    f2 = (np.sum((uni0[1] > 1))) # \n",
    "    f3 = (np.sum((uni1[1] > 1))) #\n",
    "    f4 = (num_topics_phi - len(uni0[0]))\n",
    "    f5 = (num_topics_phi_0 - len(uni1[0]))\n",
    "    return(f1, f2, f3, f4, f5)\n",
    "def asym_check(phi_true, phi_test, num_topics_phi,num_topics_phi_0):\n",
    "    m1 = asym_count_metrics_for_phi(phi_true, phi_test, simil_fun_cdist, num_topics_phi,num_topics_phi_0)\n",
    "    m2 = asym_count_metrics_for_phi(phi_true, phi_test, simil_fun_euclid, num_topics_phi,num_topics_phi_0)\n",
    "    m3 = asym_count_metrics_for_phi(phi_true, phi_test, simil_fun_jenson_shennon,num_topics_phi,num_topics_phi_0)\n",
    "    m4 = asym_count_metrics_for_phi(phi_true, phi_test, simil_fun_hellinger, num_topics_phi,num_topics_phi_0)\n",
    "    print(m1)\n",
    "    print(m2) \n",
    "    print(m3) \n",
    "    print(m4)\n",
    "    return(m1,m2,m3,m4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4490f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_experiment(\n",
    "   train_n_dw_matrix, test_n_dw_matrix, optimizer,\n",
    "   T, samples, output_path , tau , path_phi_output \n",
    "):\n",
    "\n",
    "    optimizer.iteration_callback = experiments.default_callback(\n",
    "        train_n_dw_matrix = train_n_dw_matrix,\n",
    "        test_n_dw_matrix = test_n_dw_matrix,\n",
    "        top_pmi_sizes = [5, 10, 20, 30],\n",
    "        top_avg_jaccard_sizes = [10, 50, 100, 200],\n",
    "        measure_time = False\n",
    "    )\n",
    "\n",
    "    for seed in tqdm(range(samples)):\n",
    "        realseed = seed \n",
    "        expphi,exptheta = experiments.default_sample(train_n_dw_matrix, T, realseed, optimizer,tau = tau )\n",
    "        with open(path_phi_output + str(realseed), 'wb') as resource_file:\n",
    "            pickle.dump(expphi, resource_file) \n",
    "        with open(path_phi_output + 'theta' + str(realseed ), 'wb') as resource_file:\n",
    "            pickle.dump(exptheta, resource_file)\n",
    "    optimizer.iteration_callback.save_results(output_path + str(realseed ))\n",
    "    \n",
    "    return (expphi, exptheta)\n",
    "\n",
    "def build_dataset(N_topics = 20, min_docs_in_topic = 30)\n",
    "    data = np.zeros((min_docs_in_topic*N_topics*(N_topics+1)/2, 30511))\n",
    "    train_n_dw_matrix = get_20newsgroups()\n",
    "    c2 = 0\n",
    "    for i in range(N_topics):\n",
    "        c1 = 0\n",
    "        j = 0\n",
    "        while (j < min_docs_in_topic*(i+1)):\n",
    "            if(np.array(train_n_dw_matrix)[3][c1] == i):\n",
    "                data[c2] = train_n_dw_matrix[0][c1].toarray()\n",
    "                j = j + 1\n",
    "                c2 = c2 + 1\n",
    "            c1 = c1 + 1\n",
    "    data =  sparse.csr_matrix(data)\n",
    "    with open(\"generated_data\", 'wb') as resource_file:\n",
    "        pickle.dump(data, resource_file)\n",
    "    with open(\"generated_data\", 'rb') as resource_file:\n",
    "        data = pickle.load(resource_file)\n",
    "    data = data.toarray()\n",
    "    short_indexes = []\n",
    "    for i in tqdm(range(30511)):\n",
    "        if(data[:,i].sum() < 2):\n",
    "            short_indexes.append(i)\n",
    "            data = np.delete(data,short_indexes,axis = 1)\n",
    "    data =  sparse.csr_matrix(data)\n",
    "    with open(\"generated_data_short\", 'wb') as resource_file:\n",
    "        pickle.dump(data, resource_file)\n",
    "    theta_true = np.zeros(((N_topics+1)*N_topics*min_docs_in_topic/2 ,N_topics))\n",
    "    ntopics = 0\n",
    "    docs = 0\n",
    "    i = 0\n",
    "    while(ntopics < N_topics):\n",
    "        while (docs < (ntopics+1)*(min_docs_in_topic)):\n",
    "            theta_true[i][c1] = 1\n",
    "            docs += 1\n",
    "            i += 1\n",
    "        docs = 0\n",
    "        ntopics += 1\n",
    "    return data,theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5293ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERS_COUNT = 400\n",
    "SAMPLES = 5\n",
    "regularization_list = [regularizers.Trivial()] * ITERS_COUNT\n",
    "def update(results, metric, i,j):\n",
    "    for k in range(5):\n",
    "        results[k][i][j] = metric[k]\n",
    "    return results\n",
    "def perform_main_experiment(train_n_dw_matrix, test_n_dw_matrix, optimizer,Tarray, \n",
    "                            samples, output_path , tau , path_phi_output )\n",
    "    data,theta = build_dataset()\n",
    "    for T in Tarray:\n",
    "        perform_experiment(data, None,default.Optimizer(regularization_list), T, SAMPLES, \n",
    "                           output_path = 'experiment_res_tau05_numtopics{}_'.format(T),\n",
    "                       tau = 0.5 ,path_phi_output = 'dexperiment_res_tau05_numtopics{}_'.format(T))\n",
    "\n",
    "        perform_experiment(data, None,default.Optimizer(regularization_list), T, SAMPLES, \n",
    "                           output_path = 'experiment_res_tauNone_numtopics{}_'.format(T),\n",
    "                           tau = None ,path_phi_output = 'experiment_res_tauNone_numtopics{}_'.format(T))\n",
    "    results = np.zeros((5,4,len(T)))\n",
    "    i = 0\n",
    "    for T in Tarray:\n",
    "        with open(\"experiment_res_tauNone_numtopics{}_theta0\".format(T), 'rb') as resource_file:\n",
    "              experimental_theta = pickle.load(resource_file)\n",
    "        m1,m2,m3,m4 = asym_check(theta, experimental_theta, 20, T)\n",
    "        results = update(results,m1,1,i)\n",
    "        results = update(results,m2,2,i)\n",
    "        results = update(results,m3,3,i)\n",
    "        results = update(results,m4,4,i)\n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
